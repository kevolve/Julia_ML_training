---
title: "Unit 1 - Your first day with the Julia language"
engine: julia
---

# Installation

The recommended way to install Julia is using [JuliaUp](https://github.com/JuliaLang/juliaup).
This is a program that allows you to maintain multiple Julia versions on your system simultaneously, and also helps update to newer versions as they become available.
Do not confuse this with the [package manager](https://docs.julialang.org/en/v1/stdlib/Pkg/), which is used to install packages within Julia.

For this course it is also recommended to install [Visual Studio Code](https://code.visualstudio.com/download), and the [Julia plugin for Visual Studio Code](https://www.julia-vscode.org/).
Once you have Julia installed, it's common to add a few packages to your Julia base environment.
In our case we install [IJulia.jl](https://github.com/JuliaLang/IJulia.jl) for running Julia in [Jupyter](https://jupyter.org/),
[Pluto.jl](https://github.com/fonsp/Pluto.jl) to try out [Pluto notebooks](https://plutojl.org/), [QuartoNotebookRunner.jl](https://github.com/PumasAI/QuartoNotebookRunner.jl) which integrates with [Quarto](https://quarto.org/) to create or update typeset notes like the ones you are currently reading,
and [Revise.jl](https://github.com/timholy/Revise.jl) which helps with development.

Finally, you may also want to clone this repo and instantiate a separate Julia environment for each unit of the course (we'll walk you through those steps soon).

In summary to install Julia and the supporting software, follow these steps.

1. Install Julia (and JuliaUp). Follow the instructions in the [Official Julia installation page](https://julialang.org/install/). You'll run a shell script (Mac/Linux) or use the Microsoft store on Windows. Once installed, restart your terminal, after which `juliaup` and `julia` should be available in your terminal. Then try,
    a. In the terminal run `juliaup` and you should see a short help listing.
    b. In the terminal try `julia` in the command line and you should enter the Julia REPL (Read Evaluate Print Loop). You should see a prompt, `julia> ` and try `1+1` and hit ENTER. To exit hit Ctrl+D, or `exit()` followed by ENTER. 
Note that this course was created with Julia version 1.11.5.
You can see the version of Julia that you are running when you first run Julia, or by running `versioninfo()` in the Julia REPL.
2. Install [Visual Studio Code](https://code.visualstudio.com/download) if you don't already have it installed.
3. Install the [Julia plugin for Visual Studio Code](https://www.julia-vscode.org/). See instructions [here](https://www.julia-vscode.org/docs/dev/gettingstarted/#Installing-the-Julia-extension).
4. Install the above mentioned Julia packages into your base environment. You can do this for each of the packages using the package manager as follows.
    a. Run `julia` and enter package manager mode by hitting `]`. You should see the prompt change to `(@vX.X) pkg>` where X.X is the julia version you have installed.
    b. Type `update` and ENTER to update the list of available packages from the registry.
    c. Type `add IJulia` and ENTER to install IJulia.jl. 
    d. Similarly `add Pluto` to install Pluto.jl. 
    e. Similarly `add QuartoNotebookRunner` to install QuartoNotebookRunner.jl. In each case you can use TAB to autocomplete (or hit TAB twice to see all available options).
    f. Similarly `add Revise`.


# First steps: Finding the max (loops, functions, variables, plotting)

Let's dive directly into some code. Here you can see:

* Using a Julia package with the `using` keyword. In this case, we use the `Random` package: one of the standard library packages (that come with Julia).
* Creation of a function, with the `function` keyword, and returning a result with the `return` keyword.
* A `for` loop.
* A conditional (`if`) statement.
* A few more language features. 

```{julia}
using Random

function find_max(data)
    max_value = -Inf
    for x âˆˆ data # You cna type âˆˆ by typing \in + [TAB]. Could have just used `in` as well
        if x > max_value
            max_value = x
        end
    end
    return max_value
end

Random.seed!(0)
data = rand(100)
max_value = find_max(data)
println("Found maximal value: $(max_value)")
```

You of course don't need to implement such elementary functions, since most are supplied with the language, for example:

```{julia}
maximum(data)
```

You can get help for a function by typing `? maximum` in the REPL or in Jupyter. 

Related built-in functions that may be of interest are `findmax`, `argmax`, and `max`. Similarly there are counterparts for `min` in place of max (`minimum`, `findmin`, `argmin`, and `min`). There is also `minmax`, and `extrema`. Look at the help for some of these functions and try some of the examples.  

## Some fun probability

Let's see how many times we get a new record when we scan for the maximum.

```{julia}
function find_max(data)
    max_value = -Inf
    n = 0
    for x in data
        if x > max_value
            n += 1
            max_value = x
        end
    end
    return max_value, n # Returns a tuple
end

max_value, n = find_max(data)
println("Found maximal value: $(max_value)")
println("There were $n records along the way.")
```

**If the data is i.i.d., how many times does this happen on average?**

Denote the number of records for $n$ data points by $X_n$ then,
$$
X_n = I_1 + I_2 + \ldots + I_n
$$
where,
$$
I_i =
\begin{cases}
1 & \text{if the}~i~\text{-th data point is a record}, \\
0 & \text{otherwise}.
\end{cases}
$$
Now,
$$
{\mathbb E}[X_n] = {\mathbb E}[I_1] + {\mathbb E}[I_2] + \ldots + {\mathbb E}[I_n]. 
$$
Observe that ${\mathbb E}[I_i] = {\mathbb P}(I_i = 1)$ and for statistically independent and identically distributed data points we have,
$$
{\mathbb P}(I_i = 1) = \frac{1}{i},
$$
Hence,
$$
{\mathbb E}[X_n] = h_n = \sum_{i=1}^n \frac{1}{i},
$$
the harmonic sum. It is known that
$$
h_n = \log(n) + \gamma + o(1),
$$
where $\gamma$ is the [Eulerâ€“Mascheroni constant](https://en.wikipedia.org/wiki/Euler%E2%80%93Mascheroni_constant) and $o(1)$ is a term that vanishes as $n \to \infty$. That is 
$$
h_n \approx \log(n) + \gamma.
$$

## Experimenting


Let's see it...
```{julia}
println("Î³ = ",Float64(MathConstants.Î³))  # \gamma + [TAB]
println()

# These are one-line function definitions
h(n) = sum(1/i for i in 1:n)
approx_h(n) = log(n) + MathConstants.Î³

for n in 1:10
    println(n, "\t", round(h(n), digits = 4), "\t", round(approx_h(n), digits = 4) )
end


```

Let's plot the error of the approximation as $n$ grows.

```{julia}
using Plots
err = [h(n) - approx_h(n) for n in 1:20] # This is a comprehension
scatter(1:20, err, xticks=1:20, 
        xlabel="n", ylabel = "Error", ylim = (0,0.5), legend=false)
```


Let's verify via a Monte Carlo simulation and also estimate the distribution:

```{julia}
using Statistics

records = []

data_n = 10^2
num_sims = 10^5

for s in 1:num_sims
    Random.seed!(s)
    data = rand(data_n)
    _, n = find_max(data)
    push!(records,n)
end

approx_value = approx_h(data_n)
mc_value = mean(records)

println("log($data_n) + Î³ =  $(approx_value), Monte Carlo Estimate: $(mc_value)")

histogram(records, xticks = 1:maximum(records), nbins = maximum(records), 
                normed=true, xlabel="Num Records", ylabel="Probability", legend=false)
```

# About Julia

- **Origins and History**
  - Julia was conceived in 2009 by Jeff Bezanson, Stefan Karpinski, Viral B. Shah, and Alan Edelman.
  - First publicly released in 2012, with the promise to be â€œas fast as C, as easy as Python, as flexible as Ruby, and as powerful for statistics as R.â€
  - Born out of dissatisfaction with existing technical computing languages for scientific and numerical computing.

- **Core Idea**
  - Designed for high-performance numerical and scientific computing.
  - Combines the ease of use of dynamic languages (like Python or R) with performance close to statically-typed, compiled languages (like C and Fortran).
  - Uses Just-In-Time (JIT) compilation via LLVM, meaning code is compiled to efficient machine code at runtime.

- **Key Benefits**
  - **Speed:** Approaches or matches C/Fortran in performance for many tasks.
  - **Multiple Dispatch:** Allows defining function behavior across combinations of argument types, enabling more expressive and extensible code.
  - **Rich Ecosystem:** Growing collection of packages for data science, machine learning, optimization, and visualization.
  - **Easy Syntax:** Friendly to users familiar with Python, MATLAB, or R.
  - **Metaprogramming:** Powerful features like macros for generating and transforming code.
  - **Native Parallelism:** Designed with parallel and distributed computing in mind from the start.
  - **Interoperability:** Easily calls C, Fortran, and Python libraries for leveraging existing codebases.

## Key Julia "Communities"

**Where to get help**

* [The Julia Language Slack](https://julialang.org/slack/)
* [Julia Discourse](https://discourse.julialang.org/)

**Special Domains:**

* Operations Research: [JUMP](https://jump.dev/)
* Scientific Machine Learning: [SciML](https://sciml.ai/)
* Statistics: [Julia Statistics](https://github.com/JuliaStats)
* Machine Learning: [Julia ML](https://juliaml.github.io/), [JuliaAI](https://github.com/JuliaAI)
* Climate modelling: [CliMA](https://clima.caltech.edu/)
* GPUs: [JuliaGPU](https://juliagpu.org/)

**Companies:**

* JuliaHub (used to be "Julia Computing"): [juliahub.com](https://juliahub.com/)
* PumasAI: [pumas.ai](https://pumas.ai/)
* Fugro Roames: [case study](https://juliahub.com/industries/case-studies/fugro-roames-ml)

**Books:**

* [Algorithms for Optimization](https://mitpress.mit.edu/9780262039420/algorithms-for-optimization/) by Mykel J. Kochenderfer and Tim A. Wheeler.
* [Statistics with Julia](https://statisticswithjulia.org/) by Yoni Nazarathy and Hayden Klok.

# Where to run Julia

## Shell

```
shell> echo "println(\"Hello world!\")" > temp.jl
shell> julia temp.jl
Hello world
```

Try `julia --help` to see options. 

## The REPL (Read Evaluate Print Loop)

```
shell> julia
               _
   _       _ _(_)_     |  Documentation: https://docs.julialang.org
  (_)     | (_) (_)    |
   _ _   _| |_  __ _   |  Type "?" for help, "]?" for Pkg help.
  | | | | | | |/ _` |  |
  | | |_| | | | (_| |  |  Version 1.11.5 (2025-04-14)
 _/ |\__'_|_|_|\__'_|  |  Official https://julialang.org/ release
|__/                   |

julia> 
```

At the prompt:
* Hitting `]` changes to package manager mode. 
* Hitting `;` changes to shell mode.
* Hitting `?` changes to help mode.
* Hitting `BACKSPACE` changes back to `julia>` mode.

Useful function for running code: `include`. Try: `include("temp.jl")`. Related are the `using` and `import` keywords.

## Visual Studio Code

The Julia extension makes VS-Code IDE (integrated development environment). Not quite R-Studio, but close.

- **Syntax highlighting:** Provides rich syntax highlighting for Julia code.
- **Code completion:** Offers IntelliSense, code completion, and signature help.
- **Integrated REPL:** Launch and interact with a Julia REPL directly in VS Code.
- **Debugger support:** Includes a full-featured debugger with breakpoints, stepping, and variable inspection. Not commonly used.
- **Plot pane:** View plots and figures inline within the VS Code editor.
- **Dataframe preview:** View dataframes (tables).
- **Workspace view:** Explore variables, data, and modules in your current environment.
- **Environment/project integration:** Works seamlessly with Juliaâ€™s package and environment management.
- **Notebook support:** Compatible with Jupyter notebooks via the VS Code Jupyter extension.
- **Hover/documentation:** View function/type docs and use jump-to-definition.
- **Actively maintained:** The extension is regularly updated and supported by the Julia community.

**Shortcuts:**

QQQQ-check these shortcuts

- **Start Julia REPL:** `Ctrl+Shift+P` â†’ type and select `Julia: Start REPL`
- **Execute active file in REPL:** `Ctrl+Shift+P` â†’ type and select `Julia: Execute File in REPL`
- **Send code to REPL:** `Shift+Enter`
- **Execute cell or selected lines:** `Alt+Enter`
- **Go to definition:** `F12`
- **Find references:** `Shift+F12`
- **Show documentation (hover):** `Ctrl+K Ctrl+I` (or just hover cursor over symbol)
- **Run code cell above/below:** `Ctrl+Alt+Up` / `Ctrl+Alt+Down`
- **Set breakpoint (in debug mode):** `F9`
- **Step over (in debug mode):** `F10`
- **Step into (in debug mode):** `F11`
- **Continue (in debug mode):** `F5`

## Jupyter

This is standard machine learning way of exploring and communicating ideas. 
The 'J' in "Jupyter" actually stands for "Julia". 
But the Julia community uses Jupyter a bit less (yet the course instructors are certainly avid users).

## Pluto.jl

While both Pluto.jl and Jupyter are interactive notebook environments for Julia, Pluto.jl is fundamentally reactiveâ€”changing a variable or a cell instantly updates all dependent cells throughout the notebook, ensuring consistency and minimizing hidden state issues. In contrast, Jupyter notebooks execute cells in the order chosen by the user, which can sometimes lead to hard-to-debug inconsistencies if cells are run out of order. Pluto notebooks have a more streamlined, Julia-specific design and promote reproducibility by making cell dependencies explicit, whereas Jupyter offers broader language support and a larger ecosystem of extensions but does not provide the same level of reactive interactivity or strict cell dependency mapping as Pluto.jl.

There was even a conference! [PlutoCon2021](https://plutojl.org/plutocon2021/).

```
julia> using Pluto
â”Œ Info: 
â”‚   Welcome to Pluto v0.20.8 ðŸŽˆ
â”‚   Start a notebook server using:
â”‚ 
â”‚ julia> Pluto.run()
â”‚ 
â”‚   Have a look at the FAQ:
â”‚   https://github.com/fonsp/Pluto.jl/wiki
â”” 
```

## Quarto

A modern R-markdown style environment. Used to make this course!  Has good Julia integration. See for example [Pumas Tutorials](https://tutorials.pumas.ai/) that make extensive use of Quarto.

A legacy related framework is [Weave.jl](https://github.com/JunoLab/Weave.jl).

## Integrations

Integrations within Python and R. See below.

# The Package Manager

* REPL via `]`
* The `Pkg` package

## Environment/package management in Julia

Python <> Julia correspondence?

`julia --project` OR `julia` then `]` then `activate .`

Then `instantiate` to do `pip install -r requirements.txt` but it's better

`source venv/bin/active && pip install -r requirements.txt` <> `julia --project > ] > instantiate`

# Story: Computing Square Roots (multiple dispatch, types, LLVM)

The `sqrt` function. 

```{julia}
sqrt(25)
```

Cal also use an alias for `sqrt`:

```{julia}
âˆš25 #\sqrt + [TAB]
```

```{julia}
x = sqrt(2)
@show x, x^2

x = sqrt(2f0) #32 bit float (Float32)
@show x, x^2
```

What if we try $\sqrt{-1}$?

```{julia}
sqrt(-1)
```

But if we give a complex type as input:

```{julia}
sqrt(Complex(-1))
```

In Julia a **function** (such as `sqrt` or its alias `âˆš`) can have **many methods**:

```{julia}
methods(sqrt)
```

```{julia}
using InteractiveUtils
@which sqrt(2)
```

```{julia}
@which sqrt(2.0)
```

```{julia}
@which sqrt(Ï€*im) #\pi + [Tab]
```

What if we wanted to apply square root to several/many elements together? It is common to use the `.` broadcast operator.

```{julia}
data = [i^2 for i in 0:10]
sqrt.(data) # The "." broadcating operator
```

```{julia}
x = 36
@show x^0.5
```

In Julia's source code, in [`math.jl`](https://github.com/JuliaLang/julia/blob/7b64cec5385d9099762ad7449c340eaac4fccb41/base/math.jl#L626) you'll find the following in lines 626-629:

```
@inline function sqrt(x::Union{Float32,Float64})
    x < zero(x) && throw_complex_domainerror(:sqrt, x)
    sqrt_llvm(x)
end
```


```{julia}
@code_lowered sqrt(2.5)
```

Here `sqrt_llvm()` compiles to [Low Level Virtual Machine(LLVM)](https://en.wikipedia.org/wiki/LLVM), so while many functions in Julia are actually implemented in Julia, with square roots it is better to let the underlying low level (LLVM) code handle square roots because it is later changed to assembly code; which is very fast. You can inspect this via the macros `@code_llvm` and `@code_native`.  

This will generally look the same on different computer types (LLVM is hardware agnostic):

```{julia}
@code_llvm sqrt(2.5)
```

This will look different for different computer types (cpus):

```{julia}
@code_native sqrt(2.5)
```

However, what if we wanted to do square roots via software? For example,

```{julia}
sqrt(big(10)^100)
```

What are (in principle) some [methods to compute square roots](https://en.wikipedia.org/wiki/Methods_of_computing_square_roots)? Let's look at them and implement them.

One method is the [Babylonian algorithm](https://en.wikipedia.org/wiki/Methods_of_computing_square_roots#Babylonian_method): Say we are given a positive real number $z$ and want its square root. We start with an initial guess $x_0$. We then apply the recursive step,

$$
x_{k+1} = \frac{1}{2}\Big(x_k+\frac{z}{x_k}\Big).
$$
That is, at each step the next iterate is the arithmetic mean of the previous iterate, $x_k$, and $z/x_k$. The Babylonian algorithm runs this iteration until convergence (note the default initial guess in this implementation is $z/2$):

```{julia}
function bab_sqrt(z ; init_x = z/2, verbose = false, tol = 1e-10)
    x = init_x
    while true
        verbose && println("Babylonian iterate: $x")
        next_x = 0.5*(x + z / x)
        abs(next_x - x) < tol && break
        x = next_x
    end
    x
end

bs, s = bab_sqrt(5000;verbose = true), sqrt(5000)
println("Babylonian:\t$bs\nSystem:\t\t$s")
```

## Newton's method

We can view the ancient Babylonian method as an application of the more general [Newton's method](https://en.wikipedia.org/wiki/Newton%27s_method) for solving equations. Our goal is to solve $x^2 = z$ where $z$ is given and $x$ is desired. That is define $f(x) = x^2 - z$ and we wish to find the solution of $f(x) = 0$. Newton's method iterates,

$$
x_{k+1} = x_k - \frac{f(x_k)}{f'(x_k)},
$$

based on an affine (linear) approximation of $f(\cdot)$ at the point $x_k$. Here $f'(\cdot)$ is the derivative, which in our case is $f'(x) = 2x$. So Newton's iterates for finding the square root are,

$$
x_{k+1} = x_k - \frac{x_k^2-z}{2 x_k} = \frac{x_k}{2} + \frac{z}{2x_k} = \frac{1}{2}\Big(x_k+\frac{z}{x_k}\Big).
$$

```{julia}
function newton(f, x_0::Real, der_f; Îµ = 10e-5, maxiter = 100) #\varepsilon
    x = x_0
    x_prev = x + 2Îµ
    iter = 0
    while abs(x-x_prev) â‰¥ Îµ #\ge
        x_prev = x
        x = x - f(x)/der_f(x)
        iter += 1
        if iter == maxiter 
            @info "Maximal number of iterations reached"
            break
        end
    end
    return x
end

my_root2 = newton((x)->(x^2 - 2), 3, (x)->2x)
@show my_root2
@show âˆš2
âˆš2 â‰ˆ my_root2  #\approx
```

What if we don't (easily) know the derivative?
```{julia}
f(x) = sqrt(log(sin(exp(cos((x/5)^2)*log(x/5)+x^7))) + 1)-1/2
plot(f,xlim=(0.95,1.05), xlabel = "x", ylabel="f(x)", label=false)
```

Let's use automatic differentiation (see for example Section 4.4 [here](https://deeplearningmath.org/)).

```{julia}
using ForwardDiff: derivative
auto_der = derivative(f, 0.95)

h_val = 0.0001
numer_der = (f(0.95+h_val) - f(0.95-h_val))/(2h_val)
auto_der, numer_der
```

```{julia}
function newton(f, 
                x_0::Real, 
                der_f  = (x)->derivative(f, x); 
                Îµ = 10e-8, 
                maxiter = 100)
    x = x_0
    x_prev = x + 2Îµ
    iter = 0
    while abs(x-x_prev) â‰¥ Îµ
        x_prev = x
        x = x - f(x)/der_f(x)
        iter += 1
        if iter == maxiter 
            @info "Maximal number of iterations reached"
            break
        end
    end
    return x
end

root_point = newton(f,0.95)
println("Found root: $root_point")
plot(f,xlim=(0.95,1.05), xlabel = "x", ylabel="f(x)", label=false)
scatter!([root_point], [0], color=:red, marker=:circle, label="Root found")
```

## A bit more on numerics

We now focus on floating point numbers. This is the [Double-precision floating-point format](https://en.wikipedia.org/wiki/Double-precision_floating-point_format). Before we get into the details, here are some illustrations on how it is represented in memory:

```{julia}
function pretty_print_float(x::Float64)
  bits = bitstring(x)
  println("Sign: ", bits[1])
  println("Exponent: ", bits[2:12])
  println("Significand: ", bits[13:64])
end

x = 15.324
@show typeof(x)
@show sizeof(x)
@show bitstring(x);
pretty_print_float(x)

ex = exponent(x)
sig = significand(x)
@show ex, sig
@show 2^ex * sig;
```

And this is the [Single-precision floating-point format](https://en.wikipedia.org/wiki/Single-precision_floating-point_format):

```{julia}
function pretty_print_float(x::Float32) #Notice this is a second method of pretty_print_float
  bits = bitstring(x)
  println("Sign: ", bits[1])
  println("Exponent: ", bits[2:9])
  println("Significand: ", bits[10:32])
end

x = 15.324f0
@show typeof(x)
@show sizeof(x)
@show bitstring(x);
pretty_print_float(x)

ex = exponent(x)
sig = significand(x)
@show ex, sig
@show 2^ex * sig;
```
We would ideally like to represent any number on the real line ${\mathbb R}$ via a finite number of bits with the computer. However, this is not possible and any numerical representation of a number $x \in {\mathbb R}$ is only approximated via a number $\tilde{x} \in {\mathbb F}$ where ${\mathbb F}$ is the set of  **floating point numbers**. Each such floating point number is represented as,
$$
\tilde{x} = \pm (1+f) 2^e,
$$
where $e$ is a (signed) integer called the **exponent** and $1+f$ is the **mantissa** (or **significand**). The value $f$ is represented as,
$$
f = \sum_{i=1}^d b_i 2^{-i},
$$
where $b_i \in \{0,1\}$ and $d$ is a fixed positive integer counting the number of bits used for the mantissa.

Hence the mantissa, $1+f$, lies in the range $[1,2)$ and is represented in binary form. By multiplying the equation above by $2^{-d}$ we have,
$$
f = 2^{-d} \Big(\sum_{i=1}^d b_i 2^{d-i} \Big) = 2^{-d} z.
$$
Hence $z \in \{0,1,2,\ldots,2^d-1\}$. This means that between $2^e$ and ending just before $2^e-1$ there are exactly $2^d$ evenly spaced numbers in the set ${\mathbb F}$.

Observe now that the smallest element of ${\mathbb F}$ that is greater than $1$ is $1+2^{-d}$. This motivates defining [machine epsilon](https://en.wikipedia.org/wiki/Machine_epsilon) as $\varepsilon_{\text{mach}} = 2^{-d}$.

The [IEEE 754 double precision standard](https://en.wikipedia.org/wiki/Double-precision_floating-point_format) has $d=52$ bits and single precision (`Float32`) has $d=23$ bits. Hence with `Float64` variables we have
$$
\varepsilon_{\text{mach}} = 2^{-52} \approx 2.2 \times 10^{-16}.
$$

```{julia}
@show eps() #Default is for Float64
@show eps(Float32)
@show eps(Float16)
```

```{julia}
@show 2^(-52)
@show 2^(-23)
@show 2^(-10);
```

We can suppose there is some (mathematical) function $\text{fl}: {\mathbb F} \to {\mathbb R}$ where $\text{fl}(x)$ takes a real number $x$ and maps it to the nearest $\tilde{x}$ in ${\mathbb F}$. For positive $x$ it lies in the interval $[2^e,2^{e+1})$ where the spacing between the elements is $2^{e-d}$. Hence $|\tilde{x} - x| \le \frac{1}{2} 2^{e-d}$. We can now consider the relative error between $\tilde{x}$ and $x$:
$$
\frac{|\tilde{x} - x|}{|x|} \le \frac{2^{e-d-1}}{2^e} \le \frac{1}{2} \varepsilon_{\text{mach}}.
$$

An equivalent statement states that for any $x \in {\mathbb R}$ (within the range of the exponent), there is a $\varepsilon$ where $|\varepsilon| \le \frac{1}{2} \varepsilon_{\text{mach}}$ and,
$$
\text{fl}(x) = x (1+ \varepsilon).
$$

Here is an example that looks at the irrational square roots of $\{1,2,3,\ldots,100\}$ and estimates the $\varepsilon$ associated with $\text{fl}(x)$ for each of these square roots. The example does this for `Float32` values and uses `Float64` as an approximation of the absolute truth. The two black bars are at $\pm \frac{1}{2} \varepsilon_{\text{mach}}$.

```{julia}
non_squares = setdiff(1:100,[i^2 for i in 1:100])
xÌƒ = sqrt.(Float32.(non_squares)) #x + \tilde + [TAB] 
x = sqrt.(non_squares) #Lets treat 64 bit as infinite precision
Ïµ = xÌƒ ./ x .- 1  #\epsilon + [TAB]
scatter(non_squares,Ïµ,legend=false,xlabel = "Attempt", ylabel="Approximation of Ïµ")
plot!([(x)->0.5*eps(Float32) (x)->-0.5eps(Float32)],
    c=:black,ylim=(-0.7*eps(Float32), 0.7*eps(Float32)))
```

Going back to `Float64` (double precision) we have 52 bits in the mantissa. $11$ bits for the exponent $e$ and a single sign bit. This makes $64$ bits. There are also some special values:

```{julia}
bitstring(0/0) #NaN
```

```{julia}
bitstring(1/0) #Inf
```

```{julia}
bitstring(-1/0) #-Inf
```


# Story: Computing Factorials (special functions, big numbers, more on types)

A few basic ways to compute $10! = 1\cdot 2 \cdot \ldots \cdot 10$:

```{julia}
f_a = factorial(10)
@show f_a

f_b = *(1:10...)
@show f_b

f_c = last(accumulate(*,1:10))
@show f_c

f_d = 1
for i in 1:10
    f_d *= i
end
@show f_d;

f_e = prod(1:10)
@show f_e

f_g = round(Int, exp(sum(log.(1:10))))
@show f_g;
```

Observe that,

$$
n! = 
\begin{cases}
n \cdot (n-1)! & n = 1,2,\ldots\\
1 & n = 0.
\end{cases}
$$

This is a recursive definition. Let's implement it:

```{julia}
function my_factorial(n)
    if n == 0
        return 1
    else
        return n * my_factorial(n-1)
    end
end

my_factorial(10)
```

Here is the `my_factorial()` function (written compactly).

```{julia}
my_factorial(n) = n == 0 ? 1 : n*my_factorial(n-1)

my_factorial(10)
```

Such compact writing does not change what actually happens under the hood. To see this consider both forms:

```{julia}
my_factorial1(n) = n == 0 ? 1 : n*my_factorial1(n-1)

function my_factorial2(n)
    if n == 0
        return 1
    else
        return n * my_factorial2(n-1)
    end
end;
```

Let's use Julia's `@code_lowered` macro to see how Julia parses the code into an intermediate representation (before being further compiled to LLVM). In both forms we get the exact same intermediate form.

```{julia}
@code_lowered my_factorial1(10)
```

```{julia}
@code_lowered my_factorial2(10)
```

How large can factorials we compute be? With `BigInt`, created via `big()`, there is sometimes no limit, but if we wanted to stay within the machine word size, we stay with `Int64` (with Julia `Int` is either `Int32` on "32 bit machines" or `Int64` on "64 bit machines). But even 32 bit machines support 64 bit integers (by doubling words).

Lets first use [Stirling's approximation](https://en.wikipedia.org/wiki/Stirling%27s_approximation) to get an estimate of the largest factorial we can compute with `UInt64`.

$$
n! \sim \sqrt{2 \pi} \, n^{n+\frac{1}{2}} e^{-n}
$$

```{julia}
stirling(n) = (âˆš(2Ï€*n))*(n/MathConstants.e)^n      

#An array of named tuples (note that "n!" is just a name)
[(  n! = factorial(n), 
    stirling = stirling(n), 
    ratio = round(factorial(n)/stirling(n),digits = 5)) 
    for n in 1:10]
```

Say want $n$ to not take $n!$ to be larger than,

```{julia}
typemax(UInt64), UInt(2)^64-1, float(typemax(UInt64))
```

See also the documentation about [Integers and Floating-Point Numbers](https://docs.julialang.org/en/v1/manual/integers-and-floating-point-numbers/#Integers-and-Floating-Point-Numbers).



That is solve 
$$
\sqrt{2 \pi} \, n^{n+\frac{1}{2}} e^{-n}  = 2^{64}-1.
$$

We can do a simple search, but let's use the `Roots.jl` package:

```{julia}
using Roots
stirling_sol = find_zero((x)->stirling(x)-typemax(UInt64), (1,64^64.0))
max_n = floor(Int, stirling_sol)
stirling_sol, max_n
```

Now let's see:

```{julia}
factorial(20) #ok
```

```{julia}
factorial(21) #fails
```

Indeed $n=21$ doesn't fit within the 64 bit limit.  However as suggested by the error message, using `big()` can help:

```{julia}
typeof(big(21))
```

```{julia}
factorial(big(21))
```

Just a check:

```{julia}
factorial(big(21))  == 21*factorial(big(20))
```

With (software) big integers everything goes:

```{julia}
n = 10^2
big_stuff = factorial(big(n));
num_digits = Int(ceil(log10(big_stuff))) 
println("The facotrial of $n has $num_digits decimal digits.") 
big_number = factorial(big(n))
```

Some ways to see this:

```{julia}
length(digits(big_number)), length("$big_number")
```

What about factorials of numbers that aren't positive integers?

```{julia}
factorial(6.5)
```

No, that isn't defined. But you may be looking for the [gamma](https://en.wikipedia.org/wiki/Gamma_function) special function:

$$
\Gamma(z)=\int_{0}^{\infty} x^{z-1} e^{-x} d x.
$$

```{julia}
using SpecialFunctions

gamma(6.5)
```

To feel more confident this value agrees with the integral definition of $\Gamma(\cdot)$; let's compute the integral in a very crude manner ([Riemann_sum](https://en.wikipedia.org/wiki/Riemann_sum)):

```{julia}
function my_crude_gamma(z; Î´ = 0.01, M = 50) #\delta
    integrand(x) = x^(z-1)*exp(-x) 
    x_grid = 0:Î´:M
    sum(Î´*integrand(x) for x in x_grid)
end

my_crude_gamma(6.5)
```

Or let's use a numerical integration package.

```{julia}
using QuadGK

#second output of quadgk is the error estimate, so we just take the first
my_better_crude_gamma(z) = quadgk(x -> x^(z-1)*exp(-x), 0 , Inf)[1] 

my_better_crude_gamma(6.5)
```

Now note that the gamma function is sometimes considered as the continuous version of the factorial because,
$$
\begin{aligned}
\Gamma(z+1) &=\int_{0}^{\infty} x^{z} e^{-x} d x \\
&=\left[-x^{z} e^{-x}\right]_{0}^{\infty}+\int_{0}^{\infty} z x^{z-1} e^{-x} d x \\
&=\lim _{x \rightarrow \infty}\left(-x^{z} e^{-x}\right)-\left(-0^{z} e^{-0}\right)+z \int_{0}^{\infty} x^{z-1} e^{-x} d x \\
&=z \, \Gamma(z).
\end{aligned}
$$

That is, the recursive relationship $\Gamma(z+1) = z\Gamma(z)$ holds similarly to $n! = n \cdot n!$. Further 
$$
\Gamma(1) = \int_0^\infty e^{-x} \, dx = 1.
$$
Hence we see that for integer $z$, $\Gamma(z) = (z-1)!$ or $n! = \Gamma(n+1)$. Let's try this.

```{julia}
using SpecialFunctions
[(n = n, n! = factorial(n), Î“ = gamma(n+1)) for n in 0:10]
```

The gamma function can also be extended outside of the positive reals. At some singularity points it isn't defined though.

```{julia}
@show gamma(-1.1) #here defined.
gamma(-1) #here not defined
```

Here is a plot where we exclude certain points where it isn't defined

```{julia}
using Plots, SpecialFunctions

z = setdiff(-3:0.001:4, -3:0) #setdifference to remove points where gamma() returns a NaN   
plot(z,gamma.(z), ylim=(-7,7),legend=false,xlabel="z",ylabel = "Î“(z)")
```

Can also do,

```{julia}
gamma(2+3im)
```

Related is obviously the Gamma distribution which uses the gamma function as part of the normalizing constant:

```{julia}
using Distributions

d = Gamma(2,5)
mean_d = mean(d)
plot(x->pdf(d, x), xlim = (0,30), 
            label = false, 
            xlabel="x", 
            ylabel="Gamma density", 
            fillrange = 0,
            alpha=0.5)
vline!([mean_d], color=:red, linestyle=:dash, label="Mean = $mean_d")
```

```{julia}
@doc Gamma
```
The density is,

$$
\frac{x^{\alpha-1} e^{-x/\theta}}{\Gamma(\alpha) \theta^\alpha},
$$

So in a contrived way we can extract the gamma function back out (e.g. at $x=1$ and $\theta=1$):

```{julia}
my_extracted_gamma(z) =  1 / (MathConstants.e *pdf(Gamma(z,1), 1)) 
my_extracted_gamma(6.5), gamma(6.5)
```

# More on Types

Julia has an abstract type hierarchy (a tree). At the top of the tree is the type `Any`, which encompasses every possible value in Julia. All types have a supertype (the supertype of `Any` is `Any`). Types that are not leaves of the tree have subtypes. Some types are **abstract** while others are **concrete**. One particularly distinctive feature of Julia's type system is that concrete types may not subtype each other: all concrete types are final and may only have abstract types as their supertypes.

```{julia}
x = 2.3
@show typeof(x)
@show supertype(Float64)
@show supertype(AbstractFloat)
@show supertype(Real)
@show supertype(Number)
@show supertype(Any);
```

There is an **is a** relationship:
```{julia}
isa(2.3, Number)
```

```{julia}
isa(2.3, String)
```

```{julia}
2.3 isa Float64
```

Note that `x isa T` is the same as `typeof(x) <: T`, where we say `<:` as "is a subtype of".

```{julia}
@show Float64 <: Number
@show String <: Number;
```

We can ask whether a given type is abstract or concrete.

```{julia}
@show isabstracttype(Float64)
@show isconcretetype(Float64);
```

```{julia}
@show isabstracttype(Real)
@show isconcretetype(Real);
```

Structs with undefined type paremeters are not concrete:

```{julia}
@show isconcretetype(Complex);
```

Once we provide the type parameters we do get a concrete type:

```{julia}
@show isconcretetype(Complex{Float64});
```

As mentioned, Julia has a type tree. Let's walk down from `Number`:

```{julia}
using InteractiveUtils: subtypes

function type_and_children(type, prefix = "", child_prefix = "")
    if isconcretetype(type)
        @assert isempty(subtypes(type))

        println(prefix, type, ": concrete")
    else
        println(prefix, type, isabstracttype(type) ? ": abstract" : ": parameterized")

        children = subtypes(type)
        for (i, c) in enumerate(children)
            if i == length(children)
                type_and_children(c, "$(child_prefix) â””â”€â•´", "$(child_prefix)    ")
            else
                type_and_children(c, "$(child_prefix) â”œâ”€â•´", "$(child_prefix) â”‚  ")
            end 
        end
    end
end

type_and_children(Number)
```

In Julia, you can define abstract types with the `abstract type` keywords:

```{julia}; eval = false
abstract type Number
end

abstract type Real <: Number
end

abstract type AbstractFloat <: Real
end

primitive type Float64 <: AbstractFloat
    64
end
```

## Parameterized / generic types

We've actually now seen all three types of abstract types in Julia â€“ the `abstract type`s that make up the type tree, the `Union` type, and the parameterized types (abstract `Complex` vs concrete `Complex{Float64}`).

Actually `Complex` is a shorthand. It's full type is written like this:

```{julia}; eval = false
Complex{T} where T <: Real
```

This object is of type `UnionAll`:

```{julia}
typeof(Complex)
```

You can read this like "The abstract type which is the union of the concrete types `Complex{T}` for all possible `T <: Real`" â€“ hence the shorthand `UnionAll`. Parameterised types can have bounds, like the components of complex numbers being real numbers.

```{julia}
@show Complex{Float64} <: Complex
@show isconcretetype(Complex)
@show isconcretetype(Complex{Float64});
```

Julia is pretty capable at figuring out the subtype (or subset) relationships:

```{julia}
(Complex{T} where T <: AbstractFloat) <: Complex
```

which follow from `AbstractFloat <: Real`.

You've seen other `UnionAll` types like `Vector`, `Matrix`, `Set`, `Dict`, etc.

Don't worry about this seeming complex â€“ consider this background material!

## Union types

We saw earlier that Julia has a third abstract type called `Union` which let's you reason about a finite set of (abstract or concrete) types.

```{julia}
42::Union{Int, Float64}
```

```{julia}
3.14::Union{Int, Float64}
```

```{julia}
"abc"::Union{Int, Float64}
```

`Union` can handle an arbitrary number of types, `Union{T1, T2, T3, ...}`.

As a special case `Union{T}` is just the same as `T`. We also have `Union{T, T} == T`, etc.

The union of no types at all, `Union{}`, is a special builtin type which is the opposite of `Any`. No value can exist with type `Union{}`! Sometimes `Any` is called the "top" type and `Union{}` is called the "bottom" type. It's used internally by the compiler to rule out impossible situations, but it's not something for you to worry about.

You've now seen _every possible concrete type_ and _every possible abstract type_ in all of Julia. You've also looked a functions, methods and multiple dispatch.

# Integrating with R and Python

In general, Python and R, are each more popular than Julia. Python dominates the machine learning world as well as many other fields. R dominates the statistical analysis world. Yet, packages in both of these languages of rely on low level languages for efficient computation.

In some instances Julia may play a role as such a "low level" languages, and one can consider creating packages in Julia and wrapping them in Python and R. This is the case with Julia's famous [DifferentialEquations.jl](https://github.com/SciML/DifferentialEquations.jl) package. In R it is wrapped with [diffeqr](https://github.com/SciML/diffeqr) and in Python with [diffeqpy](https://github.com/SciML/diffeqpy). 

QQQQ-diffeqr- example - QQQQQ

QQQQ-diffeqpy- example - QQQQQ

Considering the wrapping code in both these cases is instructive. QQQQ-Links-QQQQ. Let's now see how to carry integrations for ad-hoc purposes. 

## Integration with R

Let's start with an R wrapper of a Julia package: (diffeqr)[https://github.com/SciML/diffeqr]. This wrap's J

QQQQ - Example

### JuliaCall example in R

The main tool to invoke Julia from R is the `JuliaCall` R package. See it on [CRAN](https://cran.r-project.org/web/packages/JuliaCall/index.html).

QQQQ - Example

### RCall.jl example in Julia

In Julia the main tool is [RCall.jl](https://github.com/JuliaInterop/RCall.jl).

QQQQ - Example

## Integration with Python

For Python integration both tools exist together in [PythonCall.jl](https://github.com/JuliaPy/PythonCall.jl).
This gives the Python package, `juliacall` and the Julia package, `PythonCall.jl`.

### JuliaCall example in Python

QQQQ - Example

### PythonCall example in Julia

QQQQ - Example
